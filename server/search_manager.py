# assistant/search_manager.py (refactored with context builder integration)

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../")))
from config import SERPER_API_KEY
from logger_config import get_logger
from langdetect import detect
from deep_translator import GoogleTranslator
from concurrent.futures import ThreadPoolExecutor, as_completed
from latency_logger import LatencyLogger
from urllib.parse import urlparse
from playwright.sync_api import sync_playwright


from .search_to_context_builder import SearchToContextBuilder

logger = get_logger(__name__)

import re
from datetime import datetime, timedelta
from typing import List, Union

# Mapping of Thai months to month numbers
THAI_MONTHS = {
    "‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°": 1, "‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå": 2, "‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°": 3,
    "‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô": 4, "‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°": 5, "‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô": 6,
    "‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°": 7, "‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°": 8, "‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô": 9,
    "‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°": 10, "‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô": 11, "‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°": 12,
    "‡∏°.‡∏Ñ.": 1, "‡∏Å.‡∏û.": 2, "‡∏°‡∏µ.‡∏Ñ.": 3,
    "‡πÄ‡∏°.‡∏¢.": 4, "‡∏û.‡∏Ñ.": 5, "‡∏°‡∏¥.‡∏¢.": 6,
    "‡∏Å.‡∏Ñ.": 7, "‡∏™.‡∏Ñ.": 8, "‡∏Å.‡∏¢.": 9,
    "‡∏ï.‡∏Ñ.": 10, "‡∏û.‡∏¢.": 11, "‡∏ò.‡∏Ñ.": 12
}


class SearchManager:
    def __init__(self, gpt_client):
        logger.info("SearchManager initialized")
        self.serper_api_key = SERPER_API_KEY
        self.gpt_client = gpt_client
        self.context_builder = SearchToContextBuilder()

    import re

    def normalize_thai_date(self,text: str) -> str:
        if not isinstance(text, str):
            return text

        thai_months = {
            "‡∏°.‡∏Ñ.": "01", "‡∏Å.‡∏û.": "02", "‡∏°‡∏µ.‡∏Ñ.": "03", "‡πÄ‡∏°.‡∏¢.": "04",
            "‡∏û.‡∏Ñ.": "05", "‡∏°‡∏¥.‡∏¢.": "06", "‡∏Å.‡∏Ñ.": "07", "‡∏™.‡∏Ñ.": "08",
            "‡∏Å.‡∏¢.": "09", "‡∏ï.‡∏Ñ.": "10", "‡∏û.‡∏¢.": "11", "‡∏ò.‡∏Ñ.": "12",
            "‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°": "01", "‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå": "02", "‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°": "03", "‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô": "04",
            "‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°": "05", "‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô": "06", "‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°": "07", "‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°": "08",
            "‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô": "09", "‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°": "10", "‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô": "11", "‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°": "12"
        }

        pattern = r"(\d{1,2})\s?(‡∏°\.‡∏Ñ\.|‡∏Å\.‡∏û\.|‡∏°‡∏µ\.‡∏Ñ\.|‡πÄ‡∏°\.‡∏¢\.|‡∏û\.‡∏Ñ\.|‡∏°‡∏¥\.‡∏¢\.|‡∏Å\.‡∏Ñ\.|‡∏™\.‡∏Ñ\.|‡∏Å\.‡∏¢\.|‡∏ï\.‡∏Ñ\.|‡∏û\.‡∏¢\.|‡∏ò\.‡∏Ñ\.|‡∏°‡∏Å‡∏£‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏∏‡∏°‡∏†‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡πå|‡∏°‡∏µ‡∏ô‡∏≤‡∏Ñ‡∏°|‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô|‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°|‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô|‡∏Å‡∏£‡∏Å‡∏é‡∏≤‡∏Ñ‡∏°|‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏°|‡∏Å‡∏±‡∏ô‡∏¢‡∏≤‡∏¢‡∏ô|‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏°|‡∏û‡∏§‡∏®‡∏à‡∏¥‡∏Å‡∏≤‡∏¢‡∏ô|‡∏ò‡∏±‡∏ô‡∏ß‡∏≤‡∏Ñ‡∏°)\s(\d{4})"

        def replace_date(match):
            day, month_th, year = match.groups()
            month = thai_months.get(month_th)
            if not month:
                return match.group(0)  # ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏î‡πâ
            try:
                year = int(year)
                if year > 2500:
                    year -= 543
                iso = f"{year:04d}-{month}-{int(day):02d}"
                return iso
            except:
                return match.group(0)

        return re.sub(pattern, replace_date, text)

    def detect_language(self, text):
        try:
            return detect(text)
        except:
            return 'th'  # default fallback

    def translate_for_search(self,query: str, target_lang: str = "en") -> str:
        """
        Detects the language of the input query.
        If the query is in Thai and the target_lang is English, translate it.
        Otherwise, return the original query.
        """
        try:
            detected_lang = detect(query)
            if detected_lang == "th" and target_lang == "en":
                translated = GoogleTranslator(source='th', target='en').translate(query)
                return translated
            else:
                return query
        except Exception as e:
            print(f"‚ùå Translation error: {e}")
            return query
    
    def search_dual_language(self, query, top_k=5):
        lang = self.detect_language(query)

        # Primary: Original language
        if lang == 'en':
            results_primary = self.search_serper(query, top_k=top_k, lang_code='en')
        else:
            results_primary = self.search_serper(query, top_k=top_k, lang_code='th')
            results_secondary = self.search_serper(self.translate_for_search(query), top_k=top_k, lang_code="en")


        # # Secondary: Parallel search in other language
        # alt_lang = 'th' if lang == 'en' else 'en'
        # results_secondary = self.search_serper(query, top_k=top_k, lang_code=alt_lang)

        # Combine & de-duplicate by URL
        seen = set()
        combined = []
        for item in results_primary + results_secondary:
            url = item.get("link")
            if url and url not in seen:
                seen.add(url)
                combined.append(item)

        return combined[:top_k * 2]  # return up to 2x top_k entries


    def search_serper(self, query, top_k=5, lang_code="th"):
        #logger.debug(f"Enter search_serper : query = {query} : top_k={top_k}")
        url = "https://google.serper.dev/search"
        headers = {"X-API-KEY": self.serper_api_key}
        payload = {"q": query, "hl": lang_code, "gl": lang_code, "num": top_k}

        res = requests.post(url, headers=headers, json=payload)
        res.raise_for_status()
        data = res.json()
        results = data.get("organic", [])[:top_k]
        # logger.debug("calling normalize_thai_date")
        # for item in results:
        #     original_snippet = item.get("snippet", "")
        #     normalized_snippet = self.normalize_thai_date(original_snippet)
        #     item["snippet"] = normalized_snippet  # üîÅ ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà snippet ‡πÄ‡∏î‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà normalize ‡πÅ‡∏•‡πâ‡∏ß

        # logger.debug("Exit search_serper")
        return results

    def should_fetch(self, link, snippet):
        if not snippet or len(snippet) < 80:
            return True
        whitelist = ["siamsport.co.th", "tmd.go.th", "bangkokbiznews.com"]
        return any(site in link for site in whitelist)

    def fetch_with_requests(self, url):
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/114.0.0.0 Safari/537.36"
            )
        }

        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            html_content = response.text
            soup = BeautifulSoup(html_content, "html.parser")

            print(f"\n‚úÖ Successfully fetched: {url}\n")
            print("=" * 80)
            print(html_content[:3000])
            print("=" * 80)
            print("üìå Page Title:", soup.title.string if soup.title else "N/A")

            return html_content, soup

        except requests.exceptions.RequestException as e:
            print(f"\n‚ùå Failed to fetch {url}")
            print("   Reason:", str(e))
            return None, None

    def fetch_with_playwright(self, url):
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/114.0.0.0 Safari/537.36"
            )
        }
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()
                page.goto(url, timeout=15000)
                page.wait_for_load_state("networkidle")
                html_content = page.content()
                soup = BeautifulSoup(html_content, "html.parser")

                print(f"\n‚úÖ Successfully rendered with Playwright: {url}\n")
                print("=" * 80)
                print(html_content[:3000])
                print("=" * 80)
                print("üìå Page Title:", soup.title.string if soup.title else "N/A")

                browser.close()
                return html_content, soup

        except Exception as e:
            print(f"\n‚ùå Failed to render {url} with Playwright")
            print("   Reason:", str(e))
            return None, None

    def fetch_webpage_content(self, url):
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/114.0.0.0 Safari/537.36"
            )
        }
        try:
            logger.debug("Enter fetch_webpage_content")
            response = requests.get(url, headers=headers, timeout=5)
            soup = BeautifulSoup(response.text, "html.parser")
            paragraphs = soup.find_all("p")
            text = "\n".join(p.get_text() for p in paragraphs)
            logger.debug("Exit fetch_webpage_content")
            return text.strip()
        except Exception as e:
            logger.error(f"√¢¬ù¬å Error fetching {url}: {e}")
            return ""



    def fetch_all_webpages(self, urls, max_workers=5):
        results = {}
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {executor.submit(self.fetch_webpage_content, url): url for url in urls}
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                results[url] = future.result()
        return results
    
    def build_context_from_search_results(self, search_results, enable_fetch=True):
        logger.debug("Enter build_context_from_search_results")
        context_parts = []

        urls = [item.get('link') for item in search_results if item.get('link')] if enable_fetch else []
        webpage_contents = self.fetch_all_webpages(urls) if enable_fetch else {}

        for idx, item in enumerate(search_results, 1):
            title = item.get('title', '')
            snippet = item.get('snippet', '')
            link = item.get('link', '')
            page_content = webpage_contents.get(link, '') if enable_fetch else ''

            if page_content:
                page_content = page_content.split("\n")[0][:500]

            context_entry = f"""
                {idx}. {title}:
                {snippet}
                Extracted Content: {page_content if page_content else 'N/A'}
                """.strip()

            context_parts.append(context_entry)

        logger.debug("Exit build_context_from_search_results")
        return "\n\n".join(context_parts).strip()

    def build_context_from_search_results_org(self, results):
        logger.debug("Enter build_context_from_search_results")
        context = ""
        for idx, item in enumerate(results, 1):
            title = item.get("title", "")
            snippet = item.get("snippet", "")
            url = item.get("link", "")

            content = self.fetch_webpage_content(url)
            if not content:
                content = snippet

            context += f"{idx}. {title}\n{content[:1000]}\n\n"
        logger.debug(f"context={context}")
        return context.strip()
    
    def summarize_web_context(self, context_str, user_question):
        logger.debug("Enter summarize_web_context")        
        logger.debug(f"context_str={context_str}")
        # context_str = self.build_context_from_search_results(results)

        # if not context_str:
        #     return ""

        # # ‚ú® New logic: apply structured extraction and scoring
        # logger.debug(f"context_str={context_str}")
        # structured = self.context_builder.extract_entities(context_str)
        # structured_context = self.context_builder.build_gpt_context(structured)
        
        # if structured_context:
        #     return structured_context

        # Fallback: let GPT summarize as before
        #print(f"context_str={context_str[:3000]}")
        today_th = datetime.today().strftime("%-d %B %Y")  # ‡πÄ‡∏ä‡πà‡∏ô '23 May 2025'

        prompt = (
            f"‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∂‡∏Å‡∏ã‡∏∂‡πâ‡∏á\n"
            f"‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà {today_th} ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ô‡∏µ‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ '‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ' ‡∏´‡∏£‡∏∑‡∏≠ '‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î'\n\n"
            f"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ:\n{user_question}\n\n"
            f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö (‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô):\n{context_str[:3000]}\n\n"
            f"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 3-5 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î\n"
            f"- ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à\n"
            f"- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ß‡πà‡∏≤ '‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏à‡πâ‡∏≤'\n"
            f"- ‡∏´‡πâ‡∏≤‡∏°‡∏Ñ‡∏≤‡∏î‡πÄ‡∏î‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏û‡∏π‡∏î‡∏Ñ‡∏•‡∏∏‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠"
        )


        try:
            summary = self.gpt_client.chat_manager.ask_simple(prompt)
            print(f"summarized = {summary}")
            return summary.strip() if summary else ""
        except Exception as e:
            logger.error(f"‚ùå Error summarizing context: {e}")
            return ""
